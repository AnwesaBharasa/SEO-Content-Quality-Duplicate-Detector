{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-03T09:47:57.179091Z",
     "iopub.status.busy": "2025-11-03T09:47:57.178912Z",
     "iopub.status.idle": "2025-11-03T09:47:57.470824Z",
     "shell.execute_reply": "2025-11-03T09:47:57.470030Z",
     "shell.execute_reply.started": "2025-11-03T09:47:57.179068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/dataset-for-assignment/data.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T09:48:02.187049Z",
     "iopub.status.busy": "2025-11-03T09:48:02.186299Z",
     "iopub.status.idle": "2025-11-03T09:49:13.277787Z",
     "shell.execute_reply": "2025-11-03T09:49:13.276972Z",
     "shell.execute_reply.started": "2025-11-03T09:48:02.187027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.2/239.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mAll libraries installed.\n"
     ]
    }
   ],
   "source": [
    "# Install all required libraries\n",
    "!pip install -q pandas beautifulsoup4 scikit-learn nltk textstat sentence-transformers requests joblib\n",
    "print(\"All libraries installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T09:49:19.549379Z",
     "iopub.status.busy": "2025-11-03T09:49:19.548986Z",
     "iopub.status.idle": "2025-11-03T09:49:28.440678Z",
     "shell.execute_reply": "2025-11-03T09:49:28.440058Z",
     "shell.execute_reply.started": "2025-11-03T09:49:19.549351Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Data Collection & HTML Parsing ---\n",
      "Output directories created at /kaggle/working/data and /kaggle/working/models\n",
      "Loading dataset from /kaggle/input/dataset-for-assignment/data.csv...\n",
      "Loaded 81 rows.\n",
      "Parsing HTML content for all rows... This may take a moment.\n",
      "\n",
      "Successfully parsed 69 pages.\n",
      "Saved extracted content to /kaggle/working/data/extracted_content.csv\n",
      "\n",
      "Example Output:\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "5  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                               title  \\\n",
      "0                                Cyber Security Blog   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
      "5     Network Security 101: Understanding the Basics   \n",
      "\n",
      "                                           body_text  word_count  \n",
      "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326  \n",
      "1  Cybersecurity is gaining more importance globa...        1700  \n",
      "2  Home Insights Blog Posts 11 Cyber Defense Tips...        1058  \n",
      "3  Cybersecurity Best Practices CISA provides inf...         826  \n",
      "5  Home Learning center Network security Network ...        2188  \n",
      "\n",
      "--- Phase 1 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Data Collection & HTML Parsing\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(\"--- Phase 1: Data Collection & HTML Parsing ---\")\n",
    "\n",
    "# Define Kaggle Paths \n",
    "# Input path from the Kaggle dataset you added\n",
    "INPUT_CSV = '/kaggle/input/dataset-for-assignment/data.csv' \n",
    "\n",
    "# Output files will be saved in /kaggle/working/\n",
    "# We create 'data' and 'models' folders inside /kaggle/working/\n",
    "DATA_DIR = '/kaggle/working/data'\n",
    "MODELS_DIR = '/kaggle/working/models'\n",
    "OUTPUT_CSV = os.path.join(DATA_DIR, 'extracted_content.csv')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "print(f\"Output directories created at {DATA_DIR} and {MODELS_DIR}\")\n",
    "\n",
    "# Load Dataset \n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    print(f\"Error: Input file not found at {INPUT_CSV}\")\n",
    "    print(\"Please check the 'Add data' panel on the right. Is the dataset added?\")\n",
    "else:\n",
    "    print(f\"Loading dataset from {INPUT_CSV}...\")\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "    print(f\"Loaded {len(df)} rows.\")\n",
    "\n",
    "    # HTML Parsing Function \n",
    "    def parse_html_content(html_content):\n",
    "        \"\"\"\n",
    "        Parses raw HTML to extract title and clean body text.\n",
    "        Handles errors gracefully.\n",
    "        \"\"\"\n",
    "        if pd.isna(html_content) or not html_content.strip():\n",
    "            return \"No Title\", \"No Content\", 0\n",
    "\n",
    "        try:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "            # Extract Title\n",
    "            title = soup.title.string if soup.title else 'No Title Found'\n",
    "\n",
    "            # Extract Body Text: Focus on common content tags\n",
    "            content_tags = soup.find('article') or soup.find('main') or soup.find_all('p')\n",
    "            \n",
    "            if content_tags:\n",
    "                if isinstance(content_tags, list): # From find_all('p')\n",
    "                    body_text = ' '.join([tag.get_text(separator=' ') for tag in content_tags])\n",
    "                else: # From find('article') or find('main')\n",
    "                    body_text = content_tags.get_text(separator=' ')\n",
    "            else:\n",
    "                body_text = soup.body.get_text(separator=' ') if soup.body else ''\n",
    "\n",
    "            # Clean the text: remove extra whitespace\n",
    "            body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
    "            word_count = len(body_text.split())\n",
    "\n",
    "            return title, body_text, word_count\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any parsing errors\n",
    "            return \"Parsing Error\", f\"Error: {e}\", 0\n",
    "\n",
    "    # Apply Parsing and Save \n",
    "    print(\"Parsing HTML content for all rows... This may take a moment.\")\n",
    "    \n",
    "    parsed_data = df['html_content'].apply(lambda x: pd.Series(parse_html_content(x)))\n",
    "    parsed_data.columns = ['title', 'body_text', 'word_count']\n",
    "    \n",
    "    # Combine with original URL\n",
    "    df_extracted = pd.concat([df[['url']], parsed_data], axis=1)\n",
    "    \n",
    "    # Drop rows where parsing failed or yielded no content\n",
    "    df_extracted = df_extracted[df_extracted['word_count'] > 0]\n",
    "\n",
    "    # Save Output \n",
    "    df_extracted.to_csv(OUTPUT_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\nSuccessfully parsed {len(df_extracted)} pages.\")\n",
    "    print(f\"Saved extracted content to {OUTPUT_CSV}\")\n",
    "    print(\"\\nExample Output:\")\n",
    "    print(df_extracted.head())\n",
    "    print(\"\\n--- Phase 1 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T09:51:05.995467Z",
     "iopub.status.busy": "2025-11-03T09:51:05.995161Z",
     "iopub.status.idle": "2025-11-03T09:51:07.983836Z",
     "shell.execute_reply": "2025-11-03T09:51:07.983249Z",
     "shell.execute_reply.started": "2025-11-03T09:51:05.995447Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Text Preprocessing & Feature Engineering ---\n",
      "Loading extracted content from /kaggle/working/data/extracted_content.csv...\n",
      "Text cleaned (lowercased).\n",
      "Calculating readability and sentence counts...\n",
      "Calculating TF-IDF keywords...\n",
      "Top 5 keywords extracted.\n",
      "Generating sentence embeddings... This will use the GPU and should be fast.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de191b4795de4ed1b9099440e1583f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated with shape: (69, 384)\n",
      "Features saved to /kaggle/working/data/features.csv\n",
      "Embeddings saved to /kaggle/working/data/embeddings.npy\n",
      "\n",
      "Example Features Output:\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
      "4  https://nordlayer.com/learn/network-security/b...   \n",
      "\n",
      "                                               title  word_count  \\\n",
      "0                                Cyber Security Blog         326   \n",
      "1  Top 10 Cybersecurity Awareness Tips: How to St...        1700   \n",
      "2  11 Cyber Defense Tips to Stay Secure at Work a...        1058   \n",
      "3  Cybersecurity Best Practices | Cybersecurity a...         826   \n",
      "4     Network Security 101: Understanding the Basics        2188   \n",
      "\n",
      "   sentence_count  flesch_reading_ease  \\\n",
      "0               6            -6.816181   \n",
      "1              92            41.465000   \n",
      "2              62            53.262918   \n",
      "3              27            -2.538002   \n",
      "4             181            25.769415   \n",
      "\n",
      "                                       top_keywords  \n",
      "0  cyber|cybersecurity|training|management|security  \n",
      "1                      data|access|security|app|mfa  \n",
      "2              authentication|don|cyber|protect|use  \n",
      "3  cybersecurity|cyber|practices|organizations|best  \n",
      "4        network|security|monitoring|devices|access  \n",
      "\n",
      "--- Phase 2 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing & Feature Engineering\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Phase 2: Text Preprocessing & Feature Engineering ---\")\n",
    "\n",
    "# Setup and Load Data \n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True) \n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = '/kaggle/working/data'\n",
    "EXTRACTED_CSV = os.path.join(DATA_DIR, 'extracted_content.csv')\n",
    "FEATURES_CSV = os.path.join(DATA_DIR, 'features.csv')\n",
    "EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'embeddings.npy') # Save embeddings separately\n",
    "\n",
    "if not os.path.exists(EXTRACTED_CSV):\n",
    "    print(f\"Error: File not found at {EXTRACTED_CSV}. Please run Phase 1 first.\")\n",
    "else:\n",
    "    print(f\"Loading extracted content from {EXTRACTED_CSV}...\")\n",
    "    df_features = pd.read_csv(EXTRACTED_CSV)\n",
    "    df_features = df_features.dropna(subset=['body_text'])\n",
    "\n",
    "    # Clean Text\n",
    "    df_features['clean_text'] = df_features['body_text'].str.lower()\n",
    "    print(\"Text cleaned (lowercased).\")\n",
    "\n",
    "    # Extract Basic & Readability Features \n",
    "    print(\"Calculating readability and sentence counts...\")\n",
    "    df_features['sentence_count'] = df_features['clean_text'].apply(lambda x: len(nltk.sent_tokenize(str(x))))\n",
    "    df_features['flesch_reading_ease'] = df_features['clean_text'].apply(lambda x: textstat.flesch_reading_ease(str(x)))\n",
    "    \n",
    "    # Extract TF-IDF Keywords\n",
    "    print(\"Calculating TF-IDF keywords...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df_features['clean_text'])\n",
    "    \n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    \n",
    "    def get_top_keywords(row, top_n=5):\n",
    "        scores = row.toarray().flatten()\n",
    "        top_indices = scores.argsort()[-top_n:][::-1]\n",
    "        return '|'.join(feature_names[top_indices])\n",
    "\n",
    "    df_features['top_keywords'] = [get_top_keywords(row) for row in tfidf_matrix]\n",
    "    print(\"Top 5 keywords extracted.\")\n",
    "\n",
    "    # Generate Embeddings (This will use your T4 GPU!) \n",
    "    print(\"Generating sentence embeddings... This will use the GPU and should be fast.\")\n",
    "    sbert_model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "    \n",
    "    embeddings = sbert_model.encode(df_features['clean_text'].tolist(), show_progress_bar=True)\n",
    "    print(f\"Embeddings generated with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Save Features and Embeddings\n",
    "    # Don't save embeddings in the CSV (it's inefficient)\n",
    "    features_to_save = df_features.drop(columns=['body_text', 'clean_text'])\n",
    "    features_to_save.to_csv(FEATURES_CSV, index=False)\n",
    "    \n",
    "    # Save embeddings to a separate .npy file\n",
    "    np.save(EMBEDDINGS_FILE, embeddings)\n",
    "    \n",
    "    print(f\"Features saved to {FEATURES_CSV}\")\n",
    "    print(f\"Embeddings saved to {EMBEDDINGS_FILE}\")\n",
    "    print(\"\\nExample Features Output:\")\n",
    "    print(features_to_save.head())\n",
    "    print(\"\\n--- Phase 2 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:06:07.857957Z",
     "iopub.status.busy": "2025-11-03T10:06:07.857286Z",
     "iopub.status.idle": "2025-11-03T10:06:07.915122Z",
     "shell.execute_reply": "2025-11-03T10:06:07.914391Z",
     "shell.execute_reply.started": "2025-11-03T10:06:07.857932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3: Duplicate Detection ---\n",
      "Loading features and embeddings...\n",
      "Computing cosine similarity matrix...\n",
      "Found 10 duplicate pairs. Saved to /kaggle/working/data/duplicates.csv\n",
      "\n",
      "Top 5 Duplicate Pairs:\n",
      "                                                url1  \\\n",
      "0  https://en.wikipedia.org/wiki/Search_engine_op...   \n",
      "8  https://en.wikipedia.org/wiki/Artificial_intel...   \n",
      "7     https://en.wikipedia.org/wiki/Machine_learning   \n",
      "9    https://en.wikipedia.org/wiki/Digital_marketing   \n",
      "5    https://en.wikipedia.org/wiki/Content_marketing   \n",
      "\n",
      "                                                url2  similarity  \n",
      "0  https://simple.wikipedia.org/wiki/Search_engin...    0.995893  \n",
      "8  https://simple.wikipedia.org/wiki/Artificial_i...    0.995210  \n",
      "7  https://simple.wikipedia.org/wiki/Machine_lear...    0.994561  \n",
      "9  https://simple.wikipedia.org/wiki/Digital_mark...    0.980072  \n",
      "5  https://simple.wikipedia.org/wiki/Content_mark...    0.908012  \n",
      "'is_thin' column added to /kaggle/working/data/features.csv\n",
      "\n",
      "--- Duplicate & Thin Content Summary ---\n",
      "Total pages analyzed: 69\n",
      "Duplicate pairs found (>0.8 similarity): 10\n",
      "Thin content pages (<500 words): 23 (33.3%)\n",
      "\n",
      "--- Phase 3 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Detection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\n--- Phase 3: Duplicate Detection ---\")\n",
    "\n",
    "# Load Data \n",
    "DATA_DIR = '/kaggle/working/data'\n",
    "FEATURES_CSV = os.path.join(DATA_DIR, 'features.csv')\n",
    "EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'embeddings.npy')\n",
    "DUPLICATES_CSV = os.path.join(DATA_DIR, 'duplicates.csv')\n",
    "\n",
    "if not (os.path.exists(FEATURES_CSV) and os.path.exists(EMBEDDINGS_FILE)):\n",
    "    print(\"Error: Files from Phase 2 not found. Please run Phase 2 first.\")\n",
    "else:\n",
    "    print(\"Loading features and embeddings...\")\n",
    "    df_features = pd.read_csv(FEATURES_CSV)\n",
    "    embeddings = np.load(EMBEDDINGS_FILE)\n",
    "\n",
    "    # Compute Cosine Similarity \n",
    "    print(\"Computing cosine similarity matrix...\")\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Identify Duplicate Pairs\n",
    "    SIMILARITY_THRESHOLD = 0.80 \n",
    "    duplicate_pairs = []\n",
    "    \n",
    "    # Iterate over the upper triangle of the matrix\n",
    "    for i in range(len(sim_matrix)):\n",
    "        for j in range(i + 1, len(sim_matrix)):\n",
    "            if sim_matrix[i, j] > SIMILARITY_THRESHOLD:\n",
    "                pair = {\n",
    "                    'url1': df_features.iloc[i]['url'],\n",
    "                    'url2': df_features.iloc[j]['url'],\n",
    "                    'similarity': sim_matrix[i, j]\n",
    "                }\n",
    "                duplicate_pairs.append(pair)\n",
    "    \n",
    "    # Save Duplicate Pairs \n",
    "    df_duplicates = pd.DataFrame(duplicate_pairs)\n",
    "    df_duplicates = df_duplicates.sort_values(by='similarity', ascending=False)\n",
    "    df_duplicates.to_csv(DUPLICATES_CSV, index=False)\n",
    "\n",
    "    if not df_duplicates.empty:\n",
    "        print(f\"Found {len(df_duplicates)} duplicate pairs. Saved to {DUPLICATES_CSV}\")\n",
    "        print(\"\\nTop 5 Duplicate Pairs:\")\n",
    "        print(df_duplicates.head())\n",
    "    else:\n",
    "        print(\"No duplicate pairs found above threshold.\")\n",
    "\n",
    "    # Thin Content Detection \n",
    "    THIN_CONTENT_THRESHOLD = 500\n",
    "    df_features['is_thin'] = df_features['word_count'] < THIN_CONTENT_THRESHOLD\n",
    "    thin_content_count = df_features['is_thin'].sum()\n",
    "    total_pages = len(df_features)\n",
    "    thin_content_percent = (thin_content_count / total_pages) * 100\n",
    "    \n",
    "    # Update features.csv with the new 'is_thin' column\n",
    "    df_features.to_csv(FEATURES_CSV, index=False)\n",
    "    print(f\"'is_thin' column added to {FEATURES_CSV}\")\n",
    "    \n",
    "    # Report Summary\n",
    "    print(\"\\n--- Duplicate & Thin Content Summary ---\")\n",
    "    print(f\"Total pages analyzed: {total_pages}\")\n",
    "    print(f\"Duplicate pairs found (>{SIMILARITY_THRESHOLD} similarity): {len(duplicate_pairs)}\")\n",
    "    print(f\"Thin content pages (<{THIN_CONTENT_THRESHOLD} words): {thin_content_count} ({thin_content_percent:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n--- Phase 3 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:18:39.351888Z",
     "iopub.status.busy": "2025-11-03T10:18:39.351579Z",
     "iopub.status.idle": "2025-11-03T10:18:39.575248Z",
     "shell.execute_reply": "2025-11-03T10:18:39.574675Z",
     "shell.execute_reply.started": "2025-11-03T10:18:39.351868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4: Content Quality Scoring ---\n",
      "Loading features...\n",
      "Synthetic quality labels created:\n",
      "quality_label\n",
      "Low       0.521739\n",
      "Medium    0.362319\n",
      "High      0.115942\n",
      "Name: proportion, dtype: float64\n",
      "Data split: 48 train, 21 test samples.\n",
      "Training Logistic Regression model...\n",
      "\n",
      "--- Model Performance ---\n",
      "Overall Accuracy: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.00      0.00      0.00         2\n",
      "         Low       0.69      1.00      0.81        11\n",
      "      Medium       0.60      0.38      0.46         8\n",
      "\n",
      "    accuracy                           0.67        21\n",
      "   macro avg       0.43      0.46      0.43        21\n",
      "weighted avg       0.59      0.67      0.60        21\n",
      "\n",
      "Generating Confusion Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top Features ---\n",
      "               feature  importance\n",
      "2  flesch_reading_ease    0.947301\n",
      "0           word_count    0.381014\n",
      "1       sentence_count    0.335054\n",
      "\n",
      "Trained model pipeline saved to /kaggle/working/models/quality_model.pkl\n",
      "\n",
      "--- Phase 4 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Content Quality Scoring\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\n--- Phase 4: Content Quality Scoring ---\")\n",
    "\n",
    "# Load Data & Create Labels\n",
    "DATA_DIR = '/kaggle/working/data'\n",
    "MODELS_DIR = '/kaggle/working/models'\n",
    "FEATURES_CSV = os.path.join(DATA_DIR, 'features.csv')\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, 'quality_model.pkl')\n",
    "\n",
    "if not os.path.exists(FEATURES_CSV):\n",
    "    print(f\"Error: File not found at {FEATURES_CSV}. Please run Phase 3 first.\")\n",
    "else:\n",
    "    print(\"Loading features...\")\n",
    "    df_model = pd.read_csv(FEATURES_CSV)\n",
    "\n",
    "    # Create synthetic labels based on the rules\n",
    "    conditions = [\n",
    "        (df_model['word_count'] > 1500) & (df_model['flesch_reading_ease'] >= 50) & (df_model['flesch_reading_ease'] <= 70), # High\n",
    "        (df_model['word_count'] < 500) | (df_model['flesch_reading_ease'] < 30)  # Low\n",
    "    ]\n",
    "    choices = ['High', 'Low']\n",
    "    df_model['quality_label'] = np.select(conditions, choices, default='Medium')\n",
    "    \n",
    "    print(\"Synthetic quality labels created:\")\n",
    "    print(df_model['quality_label'].value_counts(normalize=True))\n",
    "\n",
    "    # Define Features (X) and Target (y) \n",
    "    feature_cols = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
    "    X = df_model[feature_cols]\n",
    "    y = df_model['quality_label']\n",
    "\n",
    "    # Train/Test Split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "    print(f\"Data split: {len(X_train)} train, {len(X_test)} test samples.\")\n",
    "\n",
    "    # Train Classification Model \n",
    "    # Use a Pipeline to combine scaling and modeling\n",
    "    print(\"Training Logistic Regression model...\")\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LogisticRegression(multi_class='ovr', random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate Model\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\n--- Model Performance ---\")\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    print(\"Generating Confusion Matrix...\")\n",
    "    %matplotlib inline\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=pipeline.classes_)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)\n",
    "    disp.plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Report Top Features\n",
    "    print(\"\\n--- Top Features ---\")\n",
    "    try:\n",
    "        model = pipeline.named_steps['model']\n",
    "        # Average absolute coefficient across classes for importance\n",
    "        importance = np.mean(np.abs(model.coef_), axis=0)\n",
    "        feature_importance = pd.DataFrame({'feature': feature_cols, 'importance': importance})\n",
    "        feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "        print(feature_importance.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine feature importance: {e}\")\n",
    "\n",
    "    # Save the Model \n",
    "    joblib.dump(pipeline, MODEL_FILE)\n",
    "    print(f\"\\nTrained model pipeline saved to {MODEL_FILE}\")\n",
    "    print(\"\\n--- Phase 4 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:21:46.172390Z",
     "iopub.status.busy": "2025-11-03T10:21:46.172094Z",
     "iopub.status.idle": "2025-11-03T10:21:47.381436Z",
     "shell.execute_reply": "2025-11-03T10:21:47.380724Z",
     "shell.execute_reply.started": "2025-11-03T10:21:46.172370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 5: Real-Time Analysis Demo ---\n",
      "Loading all assets (model, embeddings, SBERT)...\n",
      "All assets loaded and ready.\n",
      "\n",
      "Analyzing URL: https://en.wikipedia.org/wiki/Data_science...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24e0c740be04364a4b31f9cec9798aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete.\n",
      "\n",
      "--- Analysis Result ---\n",
      "{\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Data_science\",\n",
      "  \"title\": \"Data science - Wikipedia\",\n",
      "  \"word_count\": 2904,\n",
      "  \"sentence_count\": 259,\n",
      "  \"readability\": 32.88,\n",
      "  \"quality_label\": \"Low\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://en.wikipedia.org/wiki/Digital_marketing\",\n",
      "      \"similarity\": 0.8685389161109924\n",
      "    },\n",
      "    {\n",
      "      \"url\": \"https://simple.wikipedia.org/wiki/Digital_marketing\",\n",
      "      \"similarity\": 0.8504990339279175\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- Phase 5 Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Real-Time Analysis Demo\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import textstat\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Phase 5: Real-Time Analysis Demo ---\")\n",
    "\n",
    "# Load all assets one time \n",
    "print(\"Loading all assets (model, embeddings, SBERT)...\")\n",
    "DATA_DIR = '/kaggle/working/data'\n",
    "MODELS_DIR = '/kaggle/working/models'\n",
    "\n",
    "# Load the trained model pipeline\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, 'quality_model.pkl')\n",
    "model_pipeline = joblib.load(MODEL_FILE)\n",
    "\n",
    "# Load the corpus embeddings for duplicate checking\n",
    "EMBEDDINGS_FILE = os.path.join(DATA_DIR, 'embeddings.npy')\n",
    "corpus_embeddings = np.load(EMBEDDINGS_FILE)\n",
    "\n",
    "# Load the URLs corresponding to the embeddings\n",
    "FEATURES_CSV = os.path.join(DATA_DIR, 'features.csv')\n",
    "df_corpus = pd.read_csv(FEATURES_CSV)\n",
    "corpus_urls = df_corpus['url'].tolist()\n",
    "\n",
    "# Initialize the SBERT model (will use T4 GPU)\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"All assets loaded and ready.\")\n",
    "\n",
    "# Define the Real-Time Analysis Function \n",
    "def analyze_url(url):\n",
    "    \"\"\"\n",
    "    Accepts a URL, scrapes it, extracts features, scores quality,\n",
    "    and checks for duplicates against the corpus.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing URL: {url}...\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Scrape & Parse \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Use the same parsing logic as Phase 1\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else 'No Title Found'\n",
    "        content_tags = soup.find('article') or soup.find('main') or soup.find_all('p')\n",
    "        if content_tags:\n",
    "            if isinstance(content_tags, list):\n",
    "                body_text = ' '.join([tag.get_text(separator=' ') for tag in content_tags])\n",
    "            else:\n",
    "                body_text = content_tags.get_text(separator=' ')\n",
    "        else:\n",
    "            body_text = soup.body.get_text(separator=' ') if soup.body else ''\n",
    "        \n",
    "        clean_text = re.sub(r'\\s+', ' ', body_text).strip().lower()\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error scraping URL: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing content: {e}\")\n",
    "        return {\"error\": f\"Error parsing content: {e}\"}\n",
    "\n",
    "    # Extract Features\n",
    "    word_count = len(clean_text.split())\n",
    "    if word_count == 0:\n",
    "        return {\"error\": \"Could not extract meaningful content.\"}\n",
    "        \n",
    "    sentence_count = len(nltk.sent_tokenize(clean_text))\n",
    "    readability = textstat.flesch_reading_ease(clean_text)\n",
    "    is_thin = word_count < 500\n",
    "    \n",
    "    # Predict Quality\n",
    "    features_df = pd.DataFrame([[word_count, sentence_count, readability]], \n",
    "                               columns=['word_count', 'sentence_count', 'flesch_reading_ease'])\n",
    "    quality_label = model_pipeline.predict(features_df)[0]\n",
    "    \n",
    "    #  Check for Duplicates (uses GPU)\n",
    "    new_embedding = sbert_model.encode([clean_text])\n",
    "    similarities = cosine_similarity(new_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    SIMILARITY_THRESHOLD = 0.80\n",
    "    similar_to = []\n",
    "    for i, score in enumerate(similarities):\n",
    "        # Don't compare the page to itself if it's in the corpus\n",
    "        if score > SIMILARITY_THRESHOLD and corpus_urls[i] != url:\n",
    "            similar_to.append({\n",
    "                \"url\": corpus_urls[i],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "            \n",
    "    similar_to = sorted(similar_to, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    #  Format Output\n",
    "    result = {\n",
    "        \"url\": url,\n",
    "        \"title\": title,\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"readability\": round(readability, 2),\n",
    "        \"quality_label\": quality_label,\n",
    "        \"is_thin\": is_thin,\n",
    "        \"similar_to\": similar_to\n",
    "    }\n",
    "    \n",
    "    print(\"Analysis complete.\")\n",
    "    return result\n",
    "\n",
    "# Example Usage \n",
    "# Test with a stable, well-known URL\n",
    "test_url = \"https://en.wikipedia.org/wiki/Data_science\" \n",
    "result = analyze_url(test_url)\n",
    "\n",
    "# Print as formatted JSON\n",
    "print(\"\\n--- Analysis Result ---\")\n",
    "print(json.dumps(result, indent=2))\n",
    "print(\"\\n--- Phase 5 Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:33:51.412283Z",
     "iopub.status.busy": "2025-11-03T10:33:51.411767Z",
     "iopub.status.idle": "2025-11-03T10:33:53.215132Z",
     "shell.execute_reply": "2025-11-03T10:33:53.214368Z",
     "shell.execute_reply.started": "2025-11-03T10:33:51.412260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt saved to /kaggle/working/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Generate requirements.txt\n",
    "!pip freeze > /kaggle/working/requirements.txt\n",
    "print(\"requirements.txt saved to /kaggle/working/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T10:58:02.747394Z",
     "iopub.status.busy": "2025-11-03T10:58:02.746950Z",
     "iopub.status.idle": "2025-11-03T10:58:02.756023Z",
     "shell.execute_reply": "2025-11-03T10:58:02.755224Z",
     "shell.execute_reply.started": "2025-11-03T10:58:02.747369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Streamlit app directory structure...\n",
      "Copied model, embeddings, and features to streamlit_app/models/\n",
      "Streamlit directory structure created.\n"
     ]
    }
   ],
   "source": [
    "# Create Streamlit App Structure\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Creating Streamlit app directory structure...\")\n",
    "\n",
    "# 1. Define all paths\n",
    "APP_DIR = '/kaggle/working/streamlit_app'\n",
    "UTILS_DIR = os.path.join(APP_DIR, 'utils')\n",
    "MODELS_DIR_APP = os.path.join(APP_DIR, 'models') # The app's model dir\n",
    "\n",
    "# Source paths (where your files are now)\n",
    "SRC_MODEL_FILE = '/kaggle/working/models/quality_model.pkl'\n",
    "SRC_EMBEDDINGS_FILE = '/kaggle/working/data/embeddings.npy'\n",
    "SRC_FEATURES_FILE = '/kaggle/working/data/features.csv'\n",
    "\n",
    "# 2. Create directories\n",
    "os.makedirs(APP_DIR, exist_ok=True)\n",
    "os.makedirs(UTILS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR_APP, exist_ok=True)\n",
    "\n",
    "# 3. Copy the essential files into the app's structure\n",
    "# This is crucial for deployment!\n",
    "try:\n",
    "    shutil.copy(SRC_MODEL_FILE, MODELS_DIR_APP)\n",
    "    shutil.copy(SRC_EMBEDDINGS_FILE, MODELS_DIR_APP)\n",
    "    shutil.copy(SRC_FEATURES_FILE, MODELS_DIR_APP)\n",
    "    print(\"Copied model, embeddings, and features to streamlit_app/models/\")\n",
    "except Exception as e:\n",
    "    print(f\"Error copying files: {e}\")\n",
    "    print(\"Please ensure Phase 2-4 ran successfully and created the files.\")\n",
    "\n",
    "print(\"Streamlit directory structure created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T11:00:51.422861Z",
     "iopub.status.busy": "2025-11-03T11:00:51.422092Z",
     "iopub.status.idle": "2025-11-03T11:00:51.429025Z",
     "shell.execute_reply": "2025-11-03T11:00:51.428405Z",
     "shell.execute_reply.started": "2025-11-03T11:00:51.422834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/streamlit_app/utils/parser.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/streamlit_app/utils/parser.py\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_and_parse(url):\n",
    "    \"\"\"\n",
    "    Scrapes a URL and parses its content to extract\n",
    "    title and clean body text.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # --- 1. Scrape ---\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error scraping URL: {e}\")\n",
    "        raise ValueError(f\"Error scraping URL: {e}\")\n",
    "\n",
    "    # --- 2. Parse ---\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else 'No Title Found'\n",
    "        \n",
    "        content_tags = soup.find('article') or soup.find('main') or soup.find_all('p')\n",
    "        if content_tags:\n",
    "            if isinstance(content_tags, list):\n",
    "                body_text = ' '.join([tag.get_text(separator=' ') for tag in content_tags])\n",
    "            else:\n",
    "                body_text = content_tags.get_text(separator=' ')\n",
    "        else:\n",
    "            body_text = soup.body.get_text(separator=' ') if soup.body else ''\n",
    "        \n",
    "        clean_text = re.sub(r'\\s+', ' ', body_text).strip().lower()\n",
    "        \n",
    "        if not clean_text or len(clean_text.split()) == 0:\n",
    "            raise ValueError(\"Could not extract meaningful content.\")\n",
    "            \n",
    "        return title, clean_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing content: {e}\")\n",
    "        raise ValueError(f\"Error parsing content: {e}\")\n",
    "\n",
    "print(\"File 'streamlit_app/utils/parser.py' written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T11:01:12.237264Z",
     "iopub.status.busy": "2025-11-03T11:01:12.236954Z",
     "iopub.status.idle": "2025-11-03T11:01:12.242881Z",
     "shell.execute_reply": "2025-11-03T11:01:12.242111Z",
     "shell.execute_reply.started": "2025-11-03T11:01:12.237245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/streamlit_app/utils/features.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/streamlit_app/utils/features.py\n",
    "import nltk\n",
    "import textstat\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK data (Streamlit Cloud needs this)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "def extract_features(clean_text):\n",
    "    \"\"\"\n",
    "    Extracts word count, sentence count, readability, and thin content flag.\n",
    "    \"\"\"\n",
    "    word_count = len(clean_text.split())\n",
    "    sentence_count = len(nltk.sent_tokenize(clean_text))\n",
    "    readability = textstat.flesch_reading_ease(clean_text)\n",
    "    is_thin = word_count < 500\n",
    "    \n",
    "    # Create a DataFrame for the model\n",
    "    features_df = pd.DataFrame([[word_count, sentence_count, readability]], \n",
    "                               columns=['word_count', 'sentence_count', 'flesch_reading_ease'])\n",
    "    \n",
    "    feature_dict = {\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"readability\": round(readability, 2),\n",
    "        \"is_thin\": is_thin\n",
    "    }\n",
    "    \n",
    "    return features_df, feature_dict\n",
    "\n",
    "print(\"File 'streamlit_app/utils/features.py' written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T11:01:28.656614Z",
     "iopub.status.busy": "2025-11-03T11:01:28.656323Z",
     "iopub.status.idle": "2025-11-03T11:01:28.662900Z",
     "shell.execute_reply": "2025-11-03T11:01:28.662226Z",
     "shell.execute_reply.started": "2025-11-03T11:01:28.656594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/streamlit_app/utils/scorer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/streamlit_app/utils/scorer.py\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "# --- 1. Load All Models & Data ONCE ---\n",
    "# Use @st.cache_resource to load models only once\n",
    "# This is the key to a fast Streamlit app.\n",
    "\n",
    "# Define paths relative to this file\n",
    "# 'utils/' -> 'streamlit_app/' -> 'models/'\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "MODEL_FILE = os.path.join(MODELS_DIR, 'quality_model.pkl')\n",
    "EMBEDDINGS_FILE = os.path.join(MODELS_DIR, 'embeddings.npy')\n",
    "FEATURES_FILE = os.path.join(MODELS_DIR, 'features.csv')\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    \"\"\"Loads the trained ML model.\"\"\"\n",
    "    try:\n",
    "        return joblib.load(MODEL_FILE)\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "@st.cache_resource\n",
    "def load_sbert_model():\n",
    "    \"\"\"Loads the SentenceTransformer model.\"\"\"\n",
    "    try:\n",
    "        return SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading SBERT model: {e}\")\n",
    "        return None\n",
    "\n",
    "@st.cache_data\n",
    "def load_corpus_data():\n",
    "    \"\"\"Loads the corpus embeddings and features for duplicate checking.\"\"\"\n",
    "    try:\n",
    "        corpus_embeddings = np.load(EMBEDDINGS_FILE)\n",
    "        df_corpus = pd.read_csv(FEATURES_FILE)\n",
    "        corpus_urls = df_corpus['url'].tolist()\n",
    "        return corpus_embeddings, corpus_urls\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error loading corpus data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Load everything\n",
    "model_pipeline = load_model()\n",
    "sbert_model = load_sbert_model()\n",
    "corpus_embeddings, corpus_urls = load_corpus_data()\n",
    "\n",
    "# --- 2. Main Analysis Function ---\n",
    "\n",
    "def predict_quality(features_df):\n",
    "    \"\"\"Predicts quality using the loaded model.\"\"\"\n",
    "    if model_pipeline is None:\n",
    "        raise ValueError(\"Model not loaded.\")\n",
    "    \n",
    "    try:\n",
    "        quality_label = model_pipeline.predict(features_df)[0]\n",
    "        return quality_label\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error during prediction: {e}\")\n",
    "\n",
    "def check_for_duplicates(clean_text, url):\n",
    "    \"\"\"Checks for duplicates against the loaded corpus.\"\"\"\n",
    "    if sbert_model is None or corpus_embeddings is None:\n",
    "        raise ValueError(\"Corpus or SBERT model not loaded.\")\n",
    "        \n",
    "    try:\n",
    "        new_embedding = sbert_model.encode([clean_text])\n",
    "        similarities = cosine_similarity(new_embedding, corpus_embeddings)[0]\n",
    "        \n",
    "        SIMILARITY_THRESHOLD = 0.80\n",
    "        similar_to = []\n",
    "        for i, score in enumerate(similarities):\n",
    "            # Don't compare the page to itself if it's in the corpus\n",
    "            if score > SIMILARITY_THRESHOLD and corpus_urls[i] != url:\n",
    "                similar_to.append({\n",
    "                    \"url\": corpus_urls[i],\n",
    "                    \"similarity\": float(score)\n",
    "                })\n",
    "        \n",
    "        return sorted(similar_to, key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error checking duplicates: {e}\")\n",
    "\n",
    "print(\"File 'streamlit_app/utils/scorer.py' written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T11:01:43.928903Z",
     "iopub.status.busy": "2025-11-03T11:01:43.928157Z",
     "iopub.status.idle": "2025-11-03T11:01:43.934981Z",
     "shell.execute_reply": "2025-11-03T11:01:43.934237Z",
     "shell.execute_reply.started": "2025-11-03T11:01:43.928879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/streamlit_app/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/streamlit_app/app.py\n",
    "import streamlit as st\n",
    "import json\n",
    "from utils.parser import scrape_and_parse\n",
    "from utils.features import extract_features\n",
    "from utils.scorer import predict_quality, check_for_duplicates\n",
    "\n",
    "# App Title and Description\n",
    "st.set_page_config(page_title=\"SEO Content Analyzer\", layout=\"wide\")\n",
    "st.title(\"🤖 SEO Content Quality & Duplicate Detector\")\n",
    "st.markdown(\"\"\"\n",
    "This app analyzes a live URL to assess its SEO quality and check for near-duplicates \n",
    "against a pre-computed corpus. This is a demo for the LeadWalnut/CodeWalnut assignment.\n",
    "\"\"\")\n",
    "\n",
    "# URL Input \n",
    "url = st.text_input(\"Enter a URL to analyze:\", placeholder=\"https://example.com/blog-post\")\n",
    "\n",
    "# Analyze Button\n",
    "if st.button(\"Analyze Content\", type=\"primary\"):\n",
    "    if not url:\n",
    "        st.warning(\"Please enter a URL to analyze.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Analysis Pipeline \n",
    "            with st.spinner(\"Analyzing... This may take a moment...\"):\n",
    "                \n",
    "                # Scrape and Parse\n",
    "                st.subheader(\"1. Parsing Content\")\n",
    "                title, clean_text = scrape_and_parse(url)\n",
    "                st.success(f\"Successfully scraped and parsed content.\")\n",
    "                st.text(f\"Page Title: {title}\")\n",
    "\n",
    "                #  Extract Features\n",
    "                st.subheader(\"2. Extracting Features\")\n",
    "                features_df, feature_dict = extract_features(clean_text)\n",
    "                st.success(\"Features extracted.\")\n",
    "                \n",
    "                cols = st.columns(3)\n",
    "                cols[0].metric(\"Word Count\", feature_dict['word_count'])\n",
    "                cols[1].metric(\"Sentence Count\", feature_dict['sentence_count'])\n",
    "                cols[2].metric(\"Readability (Flesch)\", f\"{feature_dict['readability']:.2f}\")\n",
    "\n",
    "                #  Predict Quality\n",
    "                st.subheader(\"3. Scoring Quality\")\n",
    "                quality_label = predict_quality(features_df)\n",
    "                st.success(\"Content quality scored.\")\n",
    "                \n",
    "                if quality_label == \"High\":\n",
    "                    st.markdown(f\"**Predicted Quality: <span style='color:green; font-weight:bold;'>{quality_label}</span>**\", unsafe_allow_html=True)\n",
    "                elif quality_label == \"Medium\":\n",
    "                    st.markdown(f\"**Predicted Quality: <span style='color:orange; font-weight:bold;'>{quality_label}</span>**\", unsafe_allow_html=True)\n",
    "                else:\n",
    "                    st.markdown(f\"**Predicted Quality: <span style='color:red; font-weight:bold;'>{quality_label}</span>**\", unsafe_allow_html=True)\n",
    "                \n",
    "                st.info(f\"Thin Content (< 500 words): **{feature_dict['is_thin']}**\")\n",
    "\n",
    "                #  Check Duplicates\n",
    "                st.subheader(\"4. Checking for Duplicates\")\n",
    "                similar_to = check_for_duplicates(clean_text, url)\n",
    "                st.success(\"Duplicate check complete.\")\n",
    "                \n",
    "                if not similar_to:\n",
    "                    st.info(\"No near-duplicates found in the corpus (similarity > 80%).\")\n",
    "                else:\n",
    "                    st.warning(f\"Found {len(similar_to)} similar page(s):\")\n",
    "                    for item in similar_to:\n",
    "                        st.markdown(f\"- **URL:** `{item['url']}` \\n - **Similarity:** `{item['similarity']:.2%}`\")\n",
    "\n",
    "                #  Show Full Result\n",
    "                st.subheader(\"5. Full Analysis Result (JSON)\")\n",
    "                result = {\n",
    "                    \"url\": url,\n",
    "                    \"title\": title,\n",
    "                    \"quality_label\": quality_label,\n",
    "                    **feature_dict,\n",
    "                    \"similar_to\": similar_to\n",
    "                }\n",
    "                st.json(json.dumps(result, indent=2))\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred during analysis: {e}\")\n",
    "\n",
    "print(\"File 'streamlit_app/app.py' written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-03T11:02:14.612552Z",
     "iopub.status.busy": "2025-11-03T11:02:14.611845Z",
     "iopub.status.idle": "2025-11-03T11:02:14.617107Z",
     "shell.execute_reply": "2025-11-03T11:02:14.616502Z",
     "shell.execute_reply.started": "2025-11-03T11:02:14.612518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/streamlit_app/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/streamlit_app/requirements.txt\n",
    "streamlit\n",
    "pandas\n",
    "scikit-learn\n",
    "nltk\n",
    "textstat\n",
    "sentence-transformers\n",
    "requests\n",
    "beautifulsoup4\n",
    "joblib\n",
    "\n",
    "print(\"File 'streamlit_app/requirements.txt' written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8634695,
     "sourceId": 13590191,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
